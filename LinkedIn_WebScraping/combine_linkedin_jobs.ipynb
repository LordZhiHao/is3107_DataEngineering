{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this script is for combining the entries from linkedin jobs files - dataAnalyst_scraped.csv, dataScientist_scraped.csv, machineLearningEngineer_scraped.csv, softwareDeveloper_scraped.csv\n",
    "# import packages \n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import files\n",
    "dataAnalyst = pd.read_csv('dataAnalyst_scraped.csv')\n",
    "dataScientist = pd.read_csv('dataScientist_scraped.csv')\n",
    "machineLearningEngineer = pd.read_csv('machineLearningEngineer_scraped.csv')\n",
    "softwareDeveloper = pd.read_csv('softwareDeveloper_scraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351 entries, 0 to 350\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       351 non-null    int64  \n",
      " 1   Job_ID           351 non-null    int64  \n",
      " 2   Job_txt          320 non-null    object \n",
      " 3   company          249 non-null    object \n",
      " 4   job-title        320 non-null    object \n",
      " 5   level            320 non-null    object \n",
      " 6   location         320 non-null    object \n",
      " 7   posted-time-ago  317 non-null    object \n",
      " 8   nb_candidats     142 non-null    float64\n",
      " 9   scraping_date    351 non-null    object \n",
      " 10  posted_date      317 non-null    object \n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 30.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# check files \n",
    "dataAnalyst.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 375 entries, 0 to 374\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       375 non-null    int64  \n",
      " 1   Job_ID           375 non-null    int64  \n",
      " 2   Job_txt          258 non-null    object \n",
      " 3   company          231 non-null    object \n",
      " 4   job-title        258 non-null    object \n",
      " 5   level            258 non-null    object \n",
      " 6   location         258 non-null    object \n",
      " 7   posted-time-ago  249 non-null    object \n",
      " 8   nb_candidats     133 non-null    float64\n",
      " 9   scraping_date    375 non-null    object \n",
      " 10  posted_date      249 non-null    object \n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 32.4+ KB\n"
     ]
    }
   ],
   "source": [
    "dataScientist.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       400 non-null    int64  \n",
      " 1   Job_ID           400 non-null    int64  \n",
      " 2   Job_txt          318 non-null    object \n",
      " 3   company          296 non-null    object \n",
      " 4   job-title        318 non-null    object \n",
      " 5   level            318 non-null    object \n",
      " 6   location         318 non-null    object \n",
      " 7   posted-time-ago  306 non-null    object \n",
      " 8   nb_candidats     172 non-null    float64\n",
      " 9   scraping_date    400 non-null    object \n",
      " 10  posted_date      306 non-null    object \n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 34.5+ KB\n"
     ]
    }
   ],
   "source": [
    "machineLearningEngineer.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25 entries, 0 to 24\n",
      "Data columns (total 11 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       25 non-null     int64  \n",
      " 1   Job_ID           25 non-null     int64  \n",
      " 2   Job_txt          20 non-null     object \n",
      " 3   company          20 non-null     object \n",
      " 4   job-title        20 non-null     object \n",
      " 5   level            20 non-null     object \n",
      " 6   location         20 non-null     object \n",
      " 7   posted-time-ago  20 non-null     object \n",
      " 8   nb_candidats     8 non-null      float64\n",
      " 9   scraping_date    25 non-null     object \n",
      " 10  posted_date      20 non-null     object \n",
      "dtypes: float64(1), int64(2), object(8)\n",
      "memory usage: 2.3+ KB\n"
     ]
    }
   ],
   "source": [
    "softwareDeveloper.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unused columns - posted-time-ago, nb_candidats, scraping_date, posted_date\n",
    "dataAnalyst = dataAnalyst.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "dataScientist = dataScientist.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "machineLearningEngineer = machineLearningEngineer.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "softwareDeveloper = softwareDeveloper.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "\n",
    "# drop na values \n",
    "dataAnalyst = dataAnalyst.dropna()\n",
    "dataScientist = dataScientist.dropna()\n",
    "machineLearningEngineer = machineLearningEngineer.dropna()\n",
    "softwareDeveloper = softwareDeveloper.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform concatenation\n",
    "linkedin_jobs = pd.DataFrame(columns=dataAnalyst.columns)\n",
    "\n",
    "linkedin_jobs = pd.concat([linkedin_jobs, dataAnalyst], ignore_index=True)\n",
    "linkedin_jobs = pd.concat([linkedin_jobs, dataScientist], ignore_index=True)\n",
    "linkedin_jobs = pd.concat([linkedin_jobs, machineLearningEngineer], ignore_index=True)\n",
    "linkedin_jobs = pd.concat([linkedin_jobs, softwareDeveloper], ignore_index=True)\n",
    "\n",
    "# drop duplicated columns \n",
    "linkedin_jobs = linkedin_jobs.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job_txt</th>\n",
       "      <th>company</th>\n",
       "      <th>job-title</th>\n",
       "      <th>level</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Compliance and Access Operations Senior A...</td>\n",
       "      <td>ByteDance</td>\n",
       "      <td>Data Compliance and Access Operations Senior A...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Intelligence Analyst, Rights Protection (South...</td>\n",
       "      <td>Sportradar</td>\n",
       "      <td>Intelligence Analyst, Rights Protection (South...</td>\n",
       "      <td>Associate</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Analyst, Data Science, Health Services ...</td>\n",
       "      <td>SingHealth</td>\n",
       "      <td>Senior Analyst, Data Science, Health Services ...</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analyst - Marketing Analytics, Regional B...</td>\n",
       "      <td>Shopee</td>\n",
       "      <td>Data Analyst - Marketing Analytics, Regional B...</td>\n",
       "      <td>Entry level</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead BI Analyst Dyson Singapore, Singapore 1 m...</td>\n",
       "      <td>Dyson</td>\n",
       "      <td>Lead BI Analyst</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "      <td>Singapore, Singapore</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Job_txt     company  \\\n",
       "0  Data Compliance and Access Operations Senior A...   ByteDance   \n",
       "1  Intelligence Analyst, Rights Protection (South...  Sportradar   \n",
       "2  Senior Analyst, Data Science, Health Services ...  SingHealth   \n",
       "3  Data Analyst - Marketing Analytics, Regional B...      Shopee   \n",
       "4  Lead BI Analyst Dyson Singapore, Singapore 1 m...       Dyson   \n",
       "\n",
       "                                           job-title             level  \\\n",
       "0  Data Compliance and Access Operations Senior A...  Mid-Senior level   \n",
       "1  Intelligence Analyst, Rights Protection (South...         Associate   \n",
       "2  Senior Analyst, Data Science, Health Services ...  Mid-Senior level   \n",
       "3  Data Analyst - Marketing Analytics, Regional B...       Entry level   \n",
       "4                                    Lead BI Analyst  Mid-Senior level   \n",
       "\n",
       "               location  \n",
       "0  Singapore, Singapore  \n",
       "1  Singapore, Singapore  \n",
       "2  Singapore, Singapore  \n",
       "3  Singapore, Singapore  \n",
       "4  Singapore, Singapore  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkedin_jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_jobs.to_csv('linkedin_jobs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create function for reproducibility \n",
    "def combine_linkedin_jobs(dataAnalyst_path='dataanalyst_scraped.csv', dataScientist_path='datascientist_scraped.csv', machineLearningEngineer_path='machinelearningengineer_scraped.csv', softwareengineer_path='softwareengineer_scraped.csv', AIdeveloper_path='AIdeveloper_scraped.csv', dataEngineer_path='dataengineer_scraped.csv'):\n",
    "    dataAnalyst = pd.read_csv(dataAnalyst_path)\n",
    "    dataScientist = pd.read_csv(dataScientist_path)\n",
    "    machineLearningEngineer = pd.read_csv(machineLearningEngineer_path)\n",
    "    softwareDeveloper = pd.read_csv(softwareengineer_path)\n",
    "    AIdeveloper = pd.read_csv(AIdeveloper_path)\n",
    "    dataEngineer = pd.read_csv(dataEngineer_path)\n",
    "\n",
    "    # drop unused columns - posted-time-ago, nb_candidats, scraping_date, posted_date\n",
    "    dataAnalyst = dataAnalyst.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "    dataScientist = dataScientist.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "    machineLearningEngineer = machineLearningEngineer.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "    softwareDeveloper = softwareDeveloper.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "    AIdeveloper = AIdeveloper.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "    dataEngineer = dataEngineer.drop(['Unnamed: 0', 'Job_ID', 'posted-time-ago', 'nb_candidats', 'scraping_date', 'posted_date'], axis=1)\n",
    "    \n",
    "    # drop na values \n",
    "    dataAnalyst = dataAnalyst.dropna()\n",
    "    dataScientist = dataScientist.dropna()\n",
    "    machineLearningEngineer = machineLearningEngineer.dropna()\n",
    "    softwareDeveloper = softwareDeveloper.dropna()\n",
    "    AIdeveloper = AIdeveloper.dropna()\n",
    "    dataEngineer = dataEngineer.dropna()\n",
    "\n",
    "    # perform concatenation\n",
    "    linkedin_jobs = pd.DataFrame(columns=dataAnalyst.columns)\n",
    "    \n",
    "    linkedin_jobs = pd.concat([linkedin_jobs, dataAnalyst], ignore_index=True)\n",
    "    linkedin_jobs = pd.concat([linkedin_jobs, dataScientist], ignore_index=True)\n",
    "    linkedin_jobs = pd.concat([linkedin_jobs, machineLearningEngineer], ignore_index=True)\n",
    "    linkedin_jobs = pd.concat([linkedin_jobs, softwareDeveloper], ignore_index=True)\n",
    "    linkedin_jobs = pd.concat([linkedin_jobs, AIdeveloper], ignore_index=True)\n",
    "    linkedin_jobs = pd.concat([linkedin_jobs, dataEngineer], ignore_index=True)\n",
    "\n",
    "    # drop duplicated columns \n",
    "    linkedin_jobs = linkedin_jobs.drop_duplicates()\n",
    "\n",
    "    # export to csv\n",
    "    linkedin_jobs.to_json(\"linkedin_jobs.json\")\n",
    "    linkedin_jobs.to_csv('linkedin_jobs.csv')\n",
    "    return linkedin_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 980 entries, 0 to 1255\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   url        980 non-null    object\n",
      " 1   Job_txt    980 non-null    object\n",
      " 2   company    980 non-null    object\n",
      " 3   job-title  980 non-null    object\n",
      " 4   level      980 non-null    object\n",
      " 5   location   980 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 85.9+ KB\n"
     ]
    }
   ],
   "source": [
    "combine_linkedin_jobs().info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

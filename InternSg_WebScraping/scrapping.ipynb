{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrap URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_all_pages = []\n",
    "for i in range(1,39):\n",
    "    url = f'https://www.internsg.com/jobs/{i}/?f_p=107&f_i&filter_s#isg-top'\n",
    "    list_of_all_pages.append(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for url in list_of_all_pages:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    divs = soup.find_all('div', class_='ast-col-lg-3')\n",
    "    for div in divs:\n",
    "    # Try to find an <a> tag within the <div>\n",
    "        a_tag = div.find('a')\n",
    "        # If an <a> tag is found and it has a 'href' attribute\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "            # Extract the URL and the job title\n",
    "            url = a_tag['href']\n",
    "            job_title = a_tag.get_text().strip()\n",
    "            # Append the data to the list\n",
    "            date = 'Not Available'\n",
    "            # Try to find the next sibling 'div' which might contain the date\n",
    "            date_div = div.find_next_sibling('div', class_='ast-col-lg-1')\n",
    "            if date_div:\n",
    "                date_span = date_div.find('span', class_='text-monospace')\n",
    "                if date_span:\n",
    "                    date = date_span.get_text().strip()\n",
    "            data.append({'URL': url, 'Job Title': job_title, 'Date':date})\n",
    "\n",
    "title_url_df = pd.DataFrame(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['22 Mar', '21 Mar', '20 Mar', '18 Mar', '16 Mar', '15 Mar',\n",
       "       'Closed', '14 Mar', '13 Mar', '12 Mar', '11 Mar', '10 Mar',\n",
       "       '08 Mar', '07 Mar', '05 Mar', '04 Mar', '01 Mar', '29 Feb',\n",
       "       '28 Feb', '27 Feb', '26 Feb', '23 Feb', '22 Feb', '20 Feb',\n",
       "       '19 Feb', '15 Feb', '14 Feb', '09 Feb', '08 Feb', '07 Feb',\n",
       "       '06 Feb', '02 Feb', '01 Feb', '31 Jan', '30 Jan', '29 Jan',\n",
       "       '26 Jan', '25 Jan', '24 Jan'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_url_df['Date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_url_df.to_csv('title_url_with_date.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Srap job details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = ['Company','Designation','Date Listed','Job Type','Job Period','Profession',\n",
    "             'Industry','Location Name','Allowance / Remuneration','Company Profile',\n",
    "             'Job Description']\n",
    "jobs_info = []\n",
    "for url in title_url_df['URL']:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    dict = dict.fromkeys(col_names, '')\n",
    "    for col_name in col_names:\n",
    "        # Find the div that contains the column name\n",
    "        col_div = soup.find('div', text=col_name, class_='font-weight-bold')\n",
    "        if col_div:\n",
    "            # The actual data is in the next sibling of the parent of col_div\n",
    "            next_div = col_div.find_next_sibling()\n",
    "            if next_div:\n",
    "                # Extract the text and store it in the dictionary\n",
    "                for span in next_div.find_all('span'):\n",
    "                    span.decompose()\n",
    "                dict[col_name] = next_div.get_text(strip=True)\n",
    "    jobs_info.append(dict)\n",
    "internSG_jobs = pd.DataFrame(jobs_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "internSG_jobs.to_csv('internSG_jobs.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af3174790ae585d247763590376d478637580d04a55c0f6663c6b7ad8182cdda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
